%vim:foldmethod=marker
%{{{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Define Article %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{article}
%\usepackage{paper}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Using Packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{natbib}
\usepackage{geometry}
\geometry{
    letterpaper,
    % landscape,
    % margin=1in
}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\newtheorem{assumption}{Assumption}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
% \usepackage[cjk]{kotex}

\usepackage{listings}
\lstset{language=R,keywordstyle={\bfseries}}
% \usepackage{tikz}
\usepackage{multirow}

\usepackage{setspace}
\doublespacing
% }}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Title & Author %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Causal Inference with Ordinal Outcomes: A Semiparametric Approach}
\author{Chanhyuk Park}
\date{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
    \maketitle

\begin{abstract}
    Causal inference has become central to political science, yet much empirical research relies on ordinal outcomes such as Likert-type survey responses. This paper shows that, without strong assumptions, ordinal outcomes only allows up to scale identification of causal effects. It further demonstrates that common estimation strategies, including OLS, ordered probit, and ordered logit, generally fail to recover causal effects under model misspecification. To address this challenge, the paper introduces a new causal parameter, anchored treatment effect (ALTE), which measures effects relative to another covariate serving as an anchor. A semiparametric kernel density estimation (KDE) approach is proposed to estimate ALTE without imposing restrictive assumptions. Monte Carlo simulations show that the KDE estimator remains unbiased across diverse error distributions, whereas conventional methods fail under misspecification. An empirical application to survey data replicates results from Tomz (2020) and demonstrates that standard approaches significantly underestimate the treatment effect.
\end{abstract}

\newpage
\section{Introduction}

With growing emphasis on the causal inference framework, political science has sought on identifying the causal effect on policies and other political factors. Survey experiment, observational studies  with designs such as difference-in-difference and regression discontinuity become some of the prevalent empirical strategy in political science. Survey experiments are considered especially useful, due to the hope for clean causal identification, and during the period from 2021 to 2024, about 20\% of articles published in top 3 general political science journals (\textit{American Journal of Political Science, American Political Science Journal, and Journal of Politics}) included survey experiment as one of their empirical strategy.

Despite their appeal, however, survey experiments and related causal inference methods face challenges when the outcome of interest is measured on an ordinal scale. Many important political concepts, such as attitudes toward redistribution \citep{Alt2017a, Magni2021a}, approval of political leaders \citep{Canes-Wrone2002a, Kriner2009a}, or foreign policy preferences \citep{Scheve2001r, Mayda2005a, Tomz2020a}, are inherently latent and captured through ordered response categories. Likert-type questions with options such as \textit{strongly disagree, disagree, neither agree nor disagree, agree, strongly agree} are perhaps the most common example. Although such measures are designed to capture gradations of an underlying latent construct, they contain only information about ordering. As a result, standard causal inference tools, which typically assume cardinal or at least interval-scale outcomes, may not yield valid or interpretable estimates when applied to ordinal data.

This paper shows that due to this problem, the exact causal effects cannot be identified with the set of standard assumptions, and suggest an alternative causal parameter of anchored latent treatment effect (ALTE). Causal effects are naturally defined on the latent space, but researchers only observe the transformed ordinal categories. Due to this problem, even with a set of standard assumptions for causal inference including stable unit treatment value assumption (SUTVA), ignorability and positivity, treatment effects on the latent scale cannot be identified exactly. Thus, this paper suggests to use another covariate as an anchor and gauge the size and direction of the causal effect relative to the anchor.

A variety of strategies have been proposed to estimate causal effects with ordinal outcomes, but each comes with important limitations. One commonly used approach to deal with problem caused by ordinal outcome is ignoring the ordinal nature and treat ordinal outcomes as cardinal variables by assigning numeric labels to each responses. This allows the use of OLS and difference-in-means estimators, but the labels are arbitrary, and the implied equal spacing between categories has no substantive justification. Consequently, such estimates can at best capture directionality, and their magnitudes are uninterpretable \citep{Schroder2017a, Bond2018a, Bloem2022a}. Item response theory (IRT) models provide an alternative by leveraging multiple survey items to recover the latent variable and then estimating treatment effects. Recent hierarchical extensions integrate these steps through EM algorithms \citep{Zhou2019a, Stoetzer2025a}. While theoretically appealing, these methods require multiple indicators of the same latent construct, which are rarely available in political science surveys. Even if we have multiple items measuring a similar concept, aggregating them into one latent trait with IRT will lose the nuanced information on each items. Moreover, they often rest on distributional assumptions that compromise consistency.

Ordered logit and probit models, grounded in a latent variable framework, are another common solution. These models respect the ordinal nature of outcomes and allow for efficient estimation via maximum likelihood. Yet they too rely on strict assumptions about the distribution of error terms—logistic for ordered logit, standard normal for ordered probit. When these assumptions are violated, the resulting estimates can be not just inefficient but also inconsistent and biased, even in large samples. If the true error distribution is far away from the distributional assumptions, such bias may flip the sign of estimated treatment effects, leading researchers to draw substantively incorrect conclusions \citep{Manski1988a, Greene2010a}.

This paper makes several contributions regarding the causal inference with ordinal outcomes. First, this paper demonstrates that the exact treatment effect on latent space is only identifiable up to scale with a set of assumptions including stable unit treatment value assumption (SUTVA), ignorability, positivity. This clarifies the inherent limits of causal inference with ordinal outcomes and highlights why interpretation must be approached with caution. Second, this paper also proposes a semiparametric alternative to conventional ordered logit and ordered probit model. When distributional assumptions are violated, both ordinal regression models fails to identify the causal effect consistently. As an alternative, this paper suggest a KDE-based estimator, which does not assume a specific error distribution but remains consistent. Utilizing Kernel Density Estimation (KDE) this estimator offers a compelling balance between robustness and interpretability, making it a promising option for applied researchers.

The paper evaluates the performance of these methods using Monte Carlo simulations that vary the distribution of the latent error term. The results demonstrate that OLS estimates are highly sensitive to the cardinalization scheme, and that ordered logit and probit become biased under distributional misspecification. In contrast, the KDE-based estimator remains unbiased across a wide range of scenarios. Finally, to illustrate the substantive importance of these issues, I apply the proposed method to a replication of Tomz (2020). Whereas the original study treated ordinal responses as cardinal and interpreted the results as average treatment effects (ATE), which can be misleading. My reanalysis compares ordered logit, ordered probit, and the KDE-based estimator. The results reveal striking differences, with conventional ordinal regression models substantially underestimating the average latent treatment effect (ALTE) potentially due to model misspecification.

Taken together, these findings suggest that political science must pay greater attention to the challenges posed by ordinal outcomes. By clarifying the limits of identification and introducing a semiparametric estimator, this paper provides applied researchers with more reliable tools for causal inference when working with ordered responses—tools that are both methodologically rigorous and practically feasible.

% By offering both a diagnostic strategy and an estimation solution, this paper provides researchers with tools to improve inference in a wide class of models using ordinal outcomes—a common yet under-scrutinized challenge in political science.

% Improvement in survey design \citep{King2004a, King2007a, Chen2024a}, sensitivity analysis \citep{Bloem2022a}, and semiparametric approach \citep{Lee1992a, Lewbel2000a, Klein2002a, Liu2024a} have been proposed to mitigate some of the biases, but most of them only deal with one or two problems.

\section{Causal Inference with Ordinal Outcomes}

Political scientists has long been interested in how people form their policy preferences. In the field of international relations, determinants of domestic support for interstate conflict is one the important and widely studied topic. Recent works have found an empirical pattern between respecting human rights at home and maintaining peaceful relations abroad. That said, countries are less likely to have war if both uphold human rights compared to pairs who do not. In this vein, \cite{Tomz2020a} investigated how the human rights records of potential adversaries affect public support for war, through survey experiments in both the U.S. and the U.K.. As in usual survey experiments, individual respondents are believed to have their true opinion on using an armed forces on a unidimensional continuum, and they test if their treatment, which contains information about human rights practices of potential adversaries, can alter this opinion on average. The key dependent variable was measured with a survey question that asked respondents to answer their support for use of armed weapons against the country in 5 ordered categories: \textit{Strongly Opppose} to \textit{Strongly Favor}. This means that individual respondents should internally transformed their opinion on a continuum to one of the 5 categories.

When analyzing the data, the authors took the most common approach that cardinalization of the 5 ordered categories by putting that each categories holds numeric values from 1 to 5. By taking this common practice, the authors implicitly assume that they know how individual respondents map their true continuous opinion to the 5 categories, and that mapping looks just like \textit{Strongly Oppose} means 1 and so on. However, this assumption is problematic in 2 ways. First, the decision to use the set of numeric value of 1 to 5 is completely arbitrary and can hardly justified. Second, if the assumption is violated, than the authors conclusion on the average effect of their treatment can be ill-fated. Even if one is only interested in altering the answers themselves, meaning they only intended to change the average answer from one level to the other level, this cardinalization assumption is still problematic because the change from \textit{Strongly Oppose} to \textit{Somewhat Oppose} can hardly have equal meaning as the numerical change from 1 to 2. This paper will show that we may still have some causal parameter without this strong assumption.

\subsection{Identification Problem}
% With the growing emphasis on causal inference, political science has increasingly focused on identifying the effects of policies, institutions, and political factors. Experimental and quasi-experimental methods—most prominently survey experiments, difference-in-differences, and regression discontinuity designs—have become standard empirical strategies across subfields.
%
% Much of this research aims to estimate causal effects on abstract and subjective outcomes that cannot be observed directly. Scholars are often interested in preferences and attitudes, such as politicians’ approval ratings \citep{Canes-Wrone2002a, Kriner2009a}, support for redistribution \citep{Alt2017a, Magni2021a}, or foreign policy orientations \citep{Scheve2001r, Mayda2005a}. These outcomes are inherently latent, yet researchers generally assume that they can be represented along a unidimensional continuum—for example, arranging individuals by the strength of their support for pension reform. To capture such constructs, survey instruments typically rely on ordered response categories, most often Likert-type items (e.g., \textit{strongly disagree, disagree, neither agree nor disagree, agree, strongly agree}).
%
% The challenge is that ordinal scales contain far less information than interval or cardinal measures. They indicate only the relative ordering of responses, without conveying meaningful distances between categories. Consequently, when outcomes are ordinal, researchers face the task of recovering causal effects on the unobserved latent variable using only information about order. This limitation complicates both identification and interpretation, raising important questions about how causal inference should be conducted when the outcome of interest is measured on an ordinal scale.

Let us denote a binary treatment with $D$ and a latent continuous outcome variable of $Y^{\ast}$. This can be thought as people's true opinion or preference on a certain political issue or policy that may lie on a unidimensional continuous space. We can think of this as individuals internally decided support for using an armed forces in the case of \cite{Tomz2020a}. Then, inside individuals mind, this latent true opinion can be assumed to follow the partially linear true data-generating process (DGP) suggested below:
\begin{equation}
    Y^{\ast} = f(X, \beta) + D^{T}\tau + \varepsilon
\end{equation}
$X$ is a vector of factors that influence the preference or opinion other that the treatment $D$, and $\varepsilon$ is a latent error term where $\varepsilon \sim f(\cdot)$ and has a CDF of $F(\cdot)$. 

When asked to express their opinion or preference in a set of ordered categories, individuals may use their internal process to transform their true latent opinion or preference, $Y^{\ast}$, to the set of ordered categories. Let's denote such internal transformation process as $g$ \footnote{One may concern the inter- and intra-personal differences in $g$. In other words, each respondent may use different internal process in transforming $Y^{\ast}$ into $Y$. This hampers the identification, but the concern may be attenuated by careful survey design utilizing anchored vignettes suggested in \citep{King2004a, King2007a}. Throughout the paper, I assume everyone shares same $g$, ignoring the additional problems arising from inter- and intra-personal differences.}, and the outcome of the transformation or the observed categories as $Y = \{1, 2, \ldots, J\}$. In the usual case the categories will have labels such as \textit{Strongly Disagree / Oppose} to \textit{Strongly Favor / Agree}. We can express this relationship as:
$$
    Y = g(Y^{\ast}) = \begin{cases}
        0 & \text{if } \alpha_{-1} < Y^{\ast} \le \alpha_{0} \\
        1 & \text{if } \alpha_{0} < Y^{\ast} \le \alpha_{1} \\
        \vdots & \vdots  \\
        J & \text{if } \alpha_{J-1} < Y^{\ast} \le \alpha_{J} \\
    \end{cases}
$$, where $\alpha_{k}$ denotes the threshold points for each ordinal category. 

In this setting, one natural causal estimand is the average treatment effect (ATE) in terms of $Y^{\ast}$, which equal to $\tau$ in Equation (1). Borrowing from the potential outcome framework \citep{Rubin1974a}, let us denote the potential outcome in the latent space as $Y^{\ast}(d)$, where $d \in \{0, 1\}$ denotes the treatment status. Then the ATE can be expressed as the difference between the expectations of the two potential outcomes:
$$
    \mathbb{E}\left[Y(1)\right] - \mathbb{E}\left[Y(0)\right]
$$ 

If we can directly observe the $Y^{\ast}$, ATE on $Y^{\ast}$ can be identified, with the following set of standard causal inference assumptions regarding the latent $Y^{\ast}$. 

\begin{assumption}{SUTVA}
    A unit's potential outcomes are not affected by the treatment given to other units. $Y^{\ast}_{i} = Y^{\ast}(D_{i})$
\end{assumption}

\begin{assumption}{Ignorability}
    The treatment assignment is conditionally independent to the potential outcomes. $Y^{\ast}_{i} \perp\!\!\!\perp D_{i} \mid X_{i}$
\end{assumption}

\begin{assumption}{Positivity}
    There is non-zero probability of being treated. $\mathbb{P}\left(D_i = 1\right) > 0$
\end{assumption}

The problem is that we usually \textit{do not} observe the continuous $Y^{\ast}$ directly, but only can observe the ordinal outcomes, the transformed version of $Y^{\ast}$. Thus, the naive estimator of ATE, $\mathbb{E}\left[Y^\ast(1)\right] - \mathbb{E}\left[Y^\ast(0)\right]$ cannot be obtained directly, unless strong assumptions on $g$ or $Y^{\ast}$ are imposed.

One common practice to overcome this inability to have ATE is cardinalization. If we can safely assume that the latent variable $Y^{\ast}$ has numeric values that have the same length as the categories for $Y$, and that numeric values match with each category of $Y$, then we can recover $Y^{\ast}$ from $Y$ using this relationship. To clarify further, cardinalization using 1 to 5 of categories \textit{Strongly Disagree / Oppose} to \textit{Strongly Agree / Favor} can be expressed as following:

\begin{assumption}{Common Cardinalization}
    $$
        Y = g(Y^{\ast}) = \begin{cases}
            \text{Stronlgy Disagree} \quad & Y^\ast = 1 \\
            \vdots \\
            \text{Stronlgy Agree} \quad & Y^\ast = 5 \\
        \end{cases}
    $$ 
\end{assumption}


Cardinalization enables researcher to use usual causal inference tools such as means-difference, least squares, and difference-in-differences, because now the numerical values are assumed to capture the latent variable $Y^{\ast}$. One critical downside for this approach comes from the fact that the numerical labels are arbitrary, and theoretically any labels should work if they preserve the order. In some problems, researchers may find some numerical labels that can is believed to be the true values on the latent space, but in many cases it is hard to justify one labels to the other. For example, labeling \textit{strongly disagree, disagree, neither agree nor disagree, agree, strongly agree} as $\{1, 2, 3, 4, 5\}$ is no more reasonable than labeling them as $\{-1, 3, 15, 50, 100\}$. Thus, the size of the difference between expectations are hardly interpretable, since they depend on the labeling scheme. For example, in case of 5 point Likert scale, if we assign $\{2, 4, 6, 8, 10\}$, instead of $\{1, 2, 3, 4, 5\}$, the size of the difference will be doubled. This may significantly misleading, since the estimates we have only have very loose link with the treatment effects, and may over- or under-estimate them. Even worse, as discussed in \citet{Bond2018a} and \citet{Schroder2017a}, in most cases, there exists at least one labeling scheme that can flip the sign of the estimated causal effect. \citet{Bloem2022a} partly deals with this problem by providing a sensitivity test and a partial identification method based on the test. 

Another problem by cardinalization is it imposes strong assumption on $g$. Therefore, if $g$ is not like in Equation (2), ATE can not be identified. To see this more clearly, suppose we have conducted same experiment as \cite{Tomz2020a} but in a much smaller scale with sample of 5. Let us further assume that we know the true potential outcome on latent space $Y^{\ast}(0)$ and $Y^{\ast}(1)$ of the respondents, and there are two candidates for $g$, $g_{a}$ and $g_{b}$ such that:
$$
    Y = g_{a}(Y^{\ast}) = \begin{cases}
        \text{Strongly Disagree} &\quad -\infty < Y^\ast \le -3 \\
        \text{Somewhat Disagree} &\quad -3 < Y^\ast \le -1 \\
        \text{Neither} &\quad -1 < Y^\ast \le 0 \\
        \text{Somewhat Agree} &\quad 0 < Y^\ast \le 2 \\
        \text{Strongly Agree} &\quad 2 < Y^\ast < \infty \\
    \end{cases}
$$ 

$$
    Y = g_{b}(Y^{\ast}) = \begin{cases}
        \text{Strongly Disagree} &\quad -\infty < Y^\ast \le -5 \\
        \text{Somewhat Disagree} &\quad -5 < Y^\ast \le 0.2 \\
        \text{Neither} &\quad 0.2 < Y^\ast \le 1 \\
        \text{Somewhat Agree} &\quad 1 < Y^\ast \le 5 \\
        \text{Strongly Agree} &\quad 5 < Y^\ast < \infty \\
    \end{cases}
$$ 

    \begin{table}[ht]
        \centering
        \begin{tabular}{c|ll|ll|ll}
            \hline
            & $Y^{\ast}(0)$ & $Y^{\ast}(1)$ & $Y_{g_{a}}(0)$ & $Y_{g_{a}}(1)$ & $Y_{g_{b}}(0)$ & $Y_{g_{b}}(1)$\\
            \hline
            A & 1.37 & 0.09 & Agree Somewhat & Agree Somewhat & Agree Somewhat & Disagree Somewhat \\
            B & -0.56 & 1.71  & Neither & Agree Somewhat & Disagree Somewhat & Agree Somewhat\\
            C & 0.36 & 0.11  & Agree Somewhat & Agree Somewhat & Neither & Disagree Somewhat\\
            D & 0.63 & 2.22 & Agree Somewhat & Strongly Agree & Neither & Agree Somewhat \\
            E & 0.4 & 0.14 & Agree Somewhat & Agree Somewhat & Neither & Disagree Somewhat \\
            \hline
        \end{tabular}
    \end{table}

Table 1 shows the example data. The first column denote each respondents, the second and third columns show the latent potential outcomes $Y^{\ast}(0)$ and $Y^{\ast}(1)$, and the last four columns show observed potential outcomes generated by $g_{a}$ and $g_{b}$. Since we know both $Y^{\ast}(0)$ and $Y^{\ast}(1)$, we can calculate the ATE. 
$$
    \mathbb{E}\left[Y^\ast(1)\right] - \mathbb{E}\left[Y^\ast(0)\right] =  \frac{1.37 + (-0.56) + 0.36 + 0.63 + 0.4}{5} - \frac{0.09 + 1.71 + 0.11 + 2.22 + 0.14}{5} = 0.41
$$ 

Now suppose the data was generated based on $g_{a}$, and we put usual numeric values of 1 to 5 to each categories as in the Assumption 4. Then, the ATE based on this:
$$
    \frac{4 + 4 + 4 + 5 + 4}{5} - \frac{4 + 3 + 4 + 4 + 4}{5} = 0.4
$$ 
which is very close to the true ATE of $0.41$.

However, what if the true $g$ is not $g_{a}$ but actually $g_{b}$? Then we would have observed $Y_{g_{b}}(0)$ and $Y_{g_{b}}(1)$ and the ATE calculation with the same numeric assignment of 1 to 5 as in Assumption 4:
$$
    \frac{4 + 2 + 3 + 3 + 3}{5} - \frac{2 + 4 + 2 + 4 + 2}{5} = -0.02
$$ which is far off from the true ATE of $0.41$. 




% However, the more serious problem of cardinalization approach is the interpretation of the estimates. For instance, means-difference and difference-in-differences are comparing numerical expectations of outcomes from different groups. However, if the observed outcomes are \textit{strongly disagree, disagree, neither agree nor disagree, agree, strongly agree}, even though we have transformed them into numbers, what does the expectations of these responses, mean in terms of the latent variable $Y^{\ast}$? In a potential outcome framework, the relationship between $\mathbb{E}\left[l(Y(d)) \mid X \right]$ and $\mathbb{E}\left[Y^\ast(d) \mid X \right]$ is not clear. Furthermore, even if we safely put some meanings to the expectations, 

% Under this setting, we can still identify the ATE like we did in Theorem 1, if we have good knowledge on the transformation function $g$. This is because if we know $g$, then we can easily recover the latent outcome $Y^{\ast}$ form the observed outcome $Y$, by using the inverse of $g$. 

% \begin{theorem}{Identification of ATE when $g$ is known}
%     When the transformation function $g$ is known, we can identify ATE by recovering the latent outcome based on $g$ and the observed outcome.
%     $$
%         \mathbb{E}\left[Y^\ast(1)\right] - \mathbb{E}\left[Y^\ast(0)\right] = \mathbb{E}\left[g^{-1}(Y(1))\right] - \mathbb{E}\left[g^{-1}(Y(0))\right]
%     $$ 
% \end{theorem}
%


The example above demonstrate that, in case we do not know $g$, the identification of ATE from the common cardinalization as in Equation (2) depends almost purely on luck. However, this does not mean that we should give up on causal inference in this setting as a whole. In fact, we can still identify the causal effect up to scale. If we know the distribution of the error term in Equation (1), $\varepsilon$. Let us denote the distribution of $\varepsilon$ as $F$. Using the threshold points $\alpha_{k}$, we can write ATE as:

$$
\begin{aligned}
    \mathbb{E}\left[Y^{\ast}(0)\right] - \mathbb{E}\left[Y^{\ast}(1))\right] &= F^{-1}(\mathbb{P}\left(Y(1) \le j \right)) - F^{-1}(\mathbb{P}\left(Y(0) \le j \right)) \\
    &=F^{-1} ( \mathbb{P}\left(Y^{\ast}(0) \le \alpha_j  \right) ) - F^{-1} ( \mathbb{P}\left( Y^{\ast}(1) \le  \alpha_j \right) ) \\
    &=F^{-1} ( \mathbb{P}\left(\varepsilon +  f(X, \beta) \le \alpha_j  \right) ) - F^{-1} ( \mathbb{P}\left( \varepsilon +  f(X, \beta) +  \tau \le  \alpha_j \right) ) \\
    &=F^{-1} ( \mathbb{P}\left( \varepsilon \le \alpha_j -  f(X, \beta) \right) ) - F^{-1} ( \mathbb{P}\left(\varepsilon \le \alpha_j -  f(X, \beta) - \tau \right) ) \\
    &= \tau \\
\end{aligned}
$$ 

We can easily see that this only identify $\tau$ up to scale by multiplying $Y^{\ast}$ and $\alpha_{k}$ by $c > 0$. This can be thought of as changing the transformation function $g$, and does not change the probability $\mathbb{P}\left(Y(d) \le j\right)$.

$$
\begin{aligned}
    &F^{-1}(\mathbb{P}\left(Y(0) \le j \right)) - F^{-1}(\mathbb{P}\left(Y(1) \le j \right)) \\
    &=F_{c}^{-1} ( \mathbb{P}\left(c\cdot \varepsilon \le c\cdot\alpha_j - c\cdot f(X, \beta) \right) ) - F_{c}^{-1} ( \mathbb{P}\left(c\cdot\varepsilon \le c\cdot\alpha_j - c\cdot f(X, \beta) - c\cdot \tau \right) ) \\
    &= c\cdot\tau \\
\end{aligned}
$$ 

Thus, we only can identify the size of the treatment effect relative to a positive constant $c$. Thankfully, all the coefficients should be scaled by the same constant for the equation to be hold. This means that the treatment effect can be identified proportion to another covariate. Let us denote such covariate as an \textit{anchor}, and further denote $\beta_{a}$ as a coefficient of the anchor. Then we can define the anchored latent treatment effect as:

\begin{definition}{Anchored Latent Treatment Effect}
    $$
        \tau_{ALTE} = \frac{c\cdot \tau}{c \cdot \beta_{a}} =  \frac{\tau}{\beta_{a}}
    $$
\end{definition}

\subsection{Estimation}

To estimate the ALTE, two common approaches can be taken: 1) IRT based approaches and 2) ordered regression.



Another approach in estimating causal effect with ordinal variable is through IRT. IRT based Two-step approach first estimate the latent variable using the serious of items that were designed to measure the same concept in different angles. In the context of the model setting above, using multiple $Y$, IRT estimates the $Y^{\ast}$. After we get the estimate for the latent variable $Y^{\ast}$, then we can employ the usual causal inference tools to identify $\tau$ up to scale since it is now have numerical meanings. The second variant of IRT based approach, the hierarchical IRT is recent discussed in \citet{Stoetzer2025a}. Instead of estimating the latent variable in the first step, they effectively merge the two steps in to EM algorithm, and produces more consistent estimates of the causal effects. Since IRT based methods do not put any numerical meaning to the ordinal outcomes themselves, the estimates from the methods can be interpreted as the target causal effect up to scale. One downside of the approach is that it is advised to have at least 3 different items for IRT to work properly, but most of the political science research rely on 1 or 2 items in measuring their outcomes. Therefore, when researcher is focusing on the treatment effect on specific dimension such as opinion on gender inequality or immigrant issue, where outcomes are not usually measured with multiple items, IRT might not be a good choice.

The third approach is using ordinal regression, and by far the most common methods are ordered logit and probit regressions. Similar to the IRT based methods, both ordered logit and probit utilize the ordered nature of the outcomes measured in ordinal scales, and designed to identify the actual target causal effect in unobserved, latent space up to scale. However, these models require strong distributional assumptions on the error term in the latent space, $\varepsilon$.

\begin{assumption}{Distributional Assumption for Ordered Logit Model and Ordered Probit Model}

    The error term, $\varepsilon$ follows standard logistic distribution (ordered logit model) or standard normal distribution (ordered probit model). 
\end{assumption}

While these assumptions facilitate fast and efficient estimation through maximum likelihood, when the distributional assumptions fail, the estimates are statistically inconsistent and biased. 

Let $i$ be the index for $i$th data. To construct likelihood function, the probability of observing $Y = j$ given $X_{i}$ is:
$$
\begin{aligned}
    \mathbb{P}\left(Y = j \mid X_{i}\right) &= \mathbb{P}\left(Y^\ast \le \alpha_j\right) - \mathbb{P}\left(Y^\ast > \alpha_{j-1}\right) \\
    &=\mathbb{P}\left(\varepsilon \le \alpha_j - (f(X_{i}, \beta) + D_{i}^{T}\tau) \right) - \mathbb{P}\left(\varepsilon > \alpha_{j-1} - (f(X_{i}, \beta) + D_{i}^{T}\tau) \right)\\
    &= F(\alpha_{j} - (f(X_{i}, \beta) + D_{i}^{T}\tau) ) - F(\alpha_{j-1} - (f(X_{i}, \beta) + D_{i}^{T}\tau))
\end{aligned}
$$
, where $F(\cdot)$ is a unknown but assumed distributional function (CDF). 

Based on this, the parameters can be easily estimated with maximum likelihood.
$$
    (\beta, \tau) = \arg\max_{\beta, \tau} \mathbb{E}\left[\frac{1}{n} \sum_{i=1}^{n} \log \left( F(\alpha_{j} - (f(X_{i}, \beta) + D_{i}^{T}\tau)) - F(\alpha_{j-1} - (f(X_{i}, \beta) + D_{i}^{T}\tau)) \right) \right]
$$ 

To solve this MLE, standard ordered logit and probit models put assumption on $F(\cdot)$. For example, Ordered Probit model assume that the error term ($\varepsilon$) follows the standard normal distribution. Let $\Phi(\cdot)$ denote the standard normal distribution function. Then the MLE becomes:

$$
    (\beta, \tau) = \arg\max_{\beta, \tau} \mathbb{E}\left[\frac{1}{n} \sum_{i=1}^{n} \log \left( \Phi(\alpha_{j} - (f(X_{i}, \beta) + D_{i}^{T}\tau)) - \Phi(\alpha_{j-1} - (f(X_{i}, \beta) + D_{i}^{T}\tau)) \right) \right]
$$ 

However, it is not guaranteed that the distributional assumption that $F(\cdot) = \Phi(\cdot)$, and if they are different, the estimators from ordered probit model are to be biased and inconsistent, because it will converge to some value $(\tilde{\beta}, \tilde{\alpha}) \neq (\beta, \alpha)$ and increase in sample size does not make the two distributions closer, and therefore the estimators from two MLE will not converge to the true values \citep{Manski1988a, Greene2010a, Bond2018a}. Considering ordered logit model, logistic distribution is assumed and exactly same logic will lead to the biased and inconsistent estimation of parameters. The size and direction of the bias depend on the difference between $F(\cdot)$ and the assumed distribution, but in recent empirical works, right (positively) skewed error distributions would generally attenuate the size of the $\beta$ estimations \citep{Johnston2020a, Smits2020a}. Theoretically, the estimates from a misspecified model may have opposite sign to the true treatment effect, leading researchers toward substantially different inferences \citep{Manski1988a, Greene2010a}. This issue even harder to detect because ordered logit and ordered probit models often produce similar results. 

This paper extends the ordinal regression approach, first by introducing a diagnostic tool for the validity of the distributional assumptions for the ordered logit and probit models, and by introducing a semiparametric method that can estimate the true treatment effect without strong distributional assumptions. 


% In the next section, this paper introduces surrogate based residuals approach developed by \cite{Liu2018a}, which can be a useful diagnostic tool for detecting distributional assumption misspecification.

% These models require strong distributional assumptions; for example, Ordered Probit assumes that the conditional distribution of outcomes follows a normal distribution. While these assumptions facilitate fast and efficient estimation using maximum likelihood, many researchers have criticized them for failing to account for three key challenges in survey data: (1) variations in individual interpretation of response scales, (2) misspecification of the true outcome distribution, and (3) uncertainty arising from imprecisely measured bracketed variables.

% First, respondents may not interpret response scales uniformly. Political opinion questions are typically measured using five-point Likert scales, which are inherently subjective. Some individuals may have lower (or higher) thresholds than others, meaning that one person’s \textit{strongly agree} might be equivalent to another’s \textit{agree} \citet{King2004a}. As shown in Figure 1, although both A and B have same opinion denoted by the red arrow, they may answer differently because they interpret the scale differently.
%
% \citet{Aldrich1977a} examined interpretability issues when recovering politicians’ ideological positions from ordinal survey responses, though without directly addressing estimation concerns. \citet{King2004a} proposed the “anchored vignettes” approach to mitigate these issues at the survey design stage. This method introduces standardized example questions designed to capture respondents’ interpretations of key concepts, allowing researchers to adjust responses accordingly. \citet{King2007a} further refined the approach by developing methods to evaluate anchoring vignettes. However, a major limitation of this approach is its reliance on the assumption that respondents interpret both vignettes and primary survey questions consistently—an assumption that is difficult to test empirically.

% \section{Test for the Distributional Assumptions: Surrogate-based Residuals Approach}
%
% The researchers generally overlook the inconsistent bias in ordered logit and probit regression models partly because the absence of appropriate diagnostic tools that can test the validity of the distributional assumptions.
% % In models that do not deal with latent space, the test for the assumption on error term can be carried out by analyzing the residuals, the difference between the observed outcome and the fitted values. However, in models with latent variable space including ordered regression models, this cannot be done since we could not directly observe the true latent outcome but only observe the ordered discretized version of it. 
%
% Although statistical tests for distributional assumptions has been existed \citep{Bera1982a, Glewwe1997a, Weiss1997a}, most of them are limited to probit models because most of the tests based on the moment conditions and moment conditions for standard normal is much clearer. More recently, \citet{Li2010a} suggested a general residual analysis approach using the sign-based statistics (SBS), by collapsing ordered choices into multiple binary choices, but the result from the analysis is hard to interpret and fails to provide consistent diagnosis because the calculates residuals are dependent on the values of the covariates. 
%
% % \citet{Bloem2022a} approached the same question in a slightly different angle and introduced a sensitivity analysis for distributional assumptions. Building on \citet{Schroder2017a}, \citet{Bloem2022a} proposed a robustness test for plausible monotonic increasing transformations of the observed ordinal scale’s distribution. This framework is valuable because it allows researchers to assess how robust their findings are to certain distributional changes, such as globally concave and convex transformations or transformations with an inflection point. However, as the author notes, despite covering many theoretically plausible cases, the study remains limited to a restricted set of distributional forms.
%
% \citet{Liu2018a} extend the residual approach suggested by \citet{Li2010a}, but instead of using the sign-based statistic, it generates a surrogate variable for the error term in latent variable space and use that to test for the distributional assumptions. Since the surrogate residuals are constructed to be continuous, it shares the similar properties to that of the common residuals for continuous outcomes. Thus, one can use this surrogate residuals to either graphically diagnosis the validity of the distributional assumption or to construct numerical test including the Kolmogorov-Smirnov test or the Anderson-Darling test. 
%
% To illustrate the construction of the surrogate residuals, let's suppose a standard problem setting with latent variable space mentioned in Section 1 and further suppose that we consider a ordered regression model with an assumption that the error term in latent space follows $F(\cdot)$. Then, define $Z$ as $- (X^T\beta + D^T\tau) + \varepsilon$, where $\varepsilon$ follows the error distribution assumed by the model, $F(\cdot)$. If the specified ordered regression model is correct, the marginal distribution of $Z$ should closely follow that of the true latent outcome variable $Y^{\ast}$. Based on $Z$ and the observed outcome $Y$, then the surrogate variable $S$ is sampled from a conditional distribution of hypothetical variable $Z$ given the observed ordinal outcomes $Y$. This results in truncating $Z$ at each of the estimated threshold points by the model. To be more specific, $S$ follows below distribution:
%
% $$
%     S \sim \begin{cases}
%         Z \mid \alpha_{0} < Z \le \alpha_{1} & \text{ if } Y = 1 \\
%         Z \mid \alpha_{2} < Z \le \alpha_{2} & \text{ if } Y = 2 \\
%         \vdots & \vdots  \\
%         Z \mid \alpha_{J-1} < Z \le \alpha_{J} & \text{ if } Y = J \\
%     \end{cases}
% $$ 
%
%
% The constructed surrogate $S$ is to be a continuous variable, thus the surrogate-based residuals can be calculated just like the other continuous variable cases:
%
% $$
%     R_{S} = S - \mathbb{E}\left[ S | X \right] = S - \mathbb{E}\left[ Z | X \right]
% $$ 
%
% Since the surrogate $S$ is continuous, graphical analysis such as Q-Q plot against the assumed distribution can provide first step test for the distributional assumption and bootstrapped goodness-of-fit tests such as Kolmogorov-Smirnov test can be done \footnote{R package \lstinline{sure} provides useful tools such as functions for surrogate residual calculation and plotting.}. Kolmogorov-Smirnov test used to check the equality between the CDF of the assumed distribution and the empirical CDF of the surrogate-based residuals by summing up the distance between two distributions. If the surrogate-based residuals from the considered ordered regression model do not seem to agree with the assumed distribution, one should consider using alternative models \footnote{However, it should be noted that since this diagnosis based on (surrogate) residuals, distributional misspecification is not the only reason that can cause a bad fit. As \citet{Glewwe1997a} and \citet{Greene2010a} noted earlier, there can be multiple reasons including omitted variable with wildly different distributions or misspecification of a functional form of the latent equation. Both misspecified functional form and omitted variable are important issues require attention but for the purpose of this paper, I focus on the issues arise from the misspecification regarding distribution of the error terms.}. One may want to try alternative parametric regression models such as skewed logit distribution \citep{Nagler1994a} or a generalized version of ordered logit or probit models \citep{Johnston2020a} and pick a model with the best goodness-of-fit score using the surrogate-based residuals. 
%
% Alternative to the parametric approach rely on semiparametric approach with minimum distributional assumptions. Semiparametric approaches have been developed to mitigate the potential distributional misspecification. The literature can be roughly divided in two categories. The rank-based maximum
% score approach put minimum assumptions on distributions that can enable quantile regressions and uses that property to estimate the parameters. One downside of this approach is that it is usually the case that the convergence is very slow \citep{Manski1988a, Lee1992a}. On the other hand, Kernel-based approaches estimate the error distribution nonparametrically using kernel estimation strategy. \citet{Lewbel2000a} was one of the first to attempt relaxing both assumptions in this approach. \citet{Klein2002a} introduced a shift-restriction-based approach that uses kernel density estimation (KDE) for both cut points and regression coefficients, providing greater flexibility. In the next section, I would like to introduce a Kernel-based semiparametric approach suggested by \citet{Klein2002a}. 

% One downside of this approach is that researchers to decide which kernels to be used, and the performance largely depends on this decision.

% Some semiparametric approaches do not require input from researchers. Variants of the maximum score and maximum rank estimation are prime examples \citet{Lee1992a} extended \citet{Manski1985a}’s maximum score estimation model for binary outcomes to ordered choice settings. \citet{Liu2024a} built on \citet{Klein2002a} by applying isotonic regression and maximum rank estimation strategy instead of kernel methods. \citet{Ito2021a} leveraged Monte Carlo resampling to construct likelihoods, enabling estimation without strict distributional assumptions.  %However, these approaches primarily focus on ordered outcomes and largely overlook the challenge of bracketed variables.

\section{KDE-based Semiparametric Ordinal Regression}

Instead of assuming the distribution of the error term, \citet{Klein2002a} suggested a method that can identify the treatment effect based on the estimated error distribution. This allow us to identify the ALTE only with Assumption 1 to 4 and following assumption on the distribution of covariates.
\begin{assumption}{Differentiable Conditional Density}
    Conditional density of $X$ given $(\beta, \tau, D, Y)$ is strictly positive and differentiable. 
\end{assumption}


Let us denote the true log-likelihood as $Q$. If we can estimate $Q$ consistently, then maximum likelihood would guarantee us consistent estimation of $\tau$. Define the quasi-log-likelihood function $\hat{Q}$ as following:
\begin{equation}
    \frac{1}{n} \sum_{i = 1}^{n} \hat{m}(X_{i}) \sum_{j = 0}^{J} 1_{\{ Y = j \}} \log \left[ \hat{p}\left(Y \le j \mid X_{i}, \beta, D_{i}, \tau \right) - \hat{p}\left( Y \le j-1 \mid X_{i}, \beta, D_{i}, \tau)\right) \right]
\end{equation}
where $\hat{p}_{-1} = 0$, $\hat{p}_{J+1} = 1$, $\hat{p}$ is a kernel regression estimator of the CDF of the error term, and the trimming function $\hat{m}$ is to rule out the extreme data points where kernel regressor may not work poorly. The only part that needs estimation in $\hat{Q}$ is $\hat{p}$, so we now focus on estimation of it.

For notational ease, let's denote $f(X, \beta) + D^{T}\tau$ as $V$. By the Bayes' rule, $\mathbb{P}\left(Y \le j \mid V) \right)$ can be expressed as:
$$
    \mathbb{P}\left(Y \le j \mid V) \right) = \frac{\mathbb{P}\left(Y \le j\right) \times g_{1}(V \mid Y \le j)}{\mathbb{P}\left(Y \le j\right) \times g_{1}(V \mid Y \le j) + \mathbb{P}\left(Y > j\right) \times g_{0}(V \mid Y > j)}
$$, where $g_{0}(\cdot)$ and $g_{0}(\cdot)$ denote conditional density of $V$ given $(Y > j)$ or $(Y \le j)$ respectively. 


While both $\mathbb{P}\left(Y \le j\right)$ and $\mathbb{P}\left(Y > j\right)$ can be approximated as a sample probability, we suggest to utilize kernel density estimation (KDE) in estimating $g_{1}(\cdot)$ and $g_{0}(\cdot)$. KDE is a non-parametric method to estimate the probability density function of a random variable based on selected kernel and bandwidth. Bandwidth selection is an important step in KDE, because both estimation quality and convergence rate are depend on bandwidth. In this paper, slightly modified version of the local smoothing bandwidth developed by \citet{Abramson1982a} and \citet{Silverman1986a} is used. As an overview, this bandwidth selection method calculates weights for each data points based on pilot KDE and the final KDE bandwidth are derived based on the weights. This process helps reducing bias in estimation. 

All suggestions on the pilot bandwidth, weights function parameters, and final bandwidth are to guarantee the consistency of the $\hat{Q}$. To be more specific, $\hat{p}$ should converge to the true $\mathbb{P}$ fast enough to ensure the performance in small sample, but also must not go too fast that the derivatives of it vanishes. If $\hat{P}$ converges too fast, then the derivatives may not exist which make it harder to establish the consistency of the $\hat{Q}$ based on maximum likelihood.

The estimation of $g_{1}(\cdot)$ starts with pilot density estimation. Let $\hat{\sigma}_{1}$ be the sample standard deviation of the $V \cdot 1_{\{ Y_{k} \le j \}}$ and $n_{1} = \sum_{k} 1_{\{ Y_{k} \le j \}}$. Fix $\delta \in (0, \frac{1}{3})$, and define a pilot bandwidth $h_{p} = n^{-\gamma}$, where $\frac{1}{10} < \gamma < \frac{1}{3(3 + \delta)}$. Then the pilot KDE of $g_{1}(\cdot)$ to be:

$$
    \hat{\pi}_{1 i} = \frac{1}{n_{1}} \sum_{k \neq i} \left[ 1_{\{ Y_{k} \le j \}} \cdot K\left(\frac{V_{i} - V_{k}}{\hat{\sigma}_{1} h_{p}}\right) \cdot \frac{1}{\hat{\sigma}_{1} h_{p}} \right]
$$ 
This is essentially same as estimating the conditional density of $\mathbb{P}\left(V \mid Y \le j\right)$ based on the subsample of $V_{k}$, where $(Y_{k} \le j)$.

Next, based on the pilot density weights for the final KDE bandwidth are calculated for each data points. These weights control the rate at which final bandwidth converges to 0 as $n \to \infty$. In other words, the weight functions can stretch the final bandwidth for some data points if data points around them are relatively scare area and thus need more smoothing (i.e. tail area), but at the same time prevent the final bandwidth from being too wide. 

First, estimated local smoothing parameter $\hat{l}_{1 i}$ is defined as:
$$
    \hat{l}_{1 i} = \frac{\hat{\pi}_{1 i}}{\hat{m}_{1}}
$$, where $\hat{m}_{1}$ is a geometric mean of the $\hat{\pi}_{1 i}$. 
This widens the final bandwidth in regions where the estimated pilot density is relatively low. This can be especially helpful in capturing tail regions.

To control the bandwidth to be too wide and thus converges too fast, define the estimated damping function, $\hat{d}_{1}$ for $l > 0$ which approximates the indicator function $1_{\{ l > a_{n_{1}} \}}$. Set $a_{n_{1}} \propto [\ln n_{1}]^{-1}$ and choose $\epsilon \in (0, \frac{1}{40} - \frac{\delta}{20})$ gives:
$$
    \hat{d}_{1} = \frac{1}{1 + \exp \left( - n_{1}^\epsilon [l - a_{n_{1}}] \right)}
$$

Lastly, calculate the weights for the final KDE, $w_{1i}$ with parameters above:
$$
    w_{1i} = \hat{\sigma_{1}} \left[ \hat{l}_{1 i} \hat{d}_{1 i} + a_{n_{1}} [1 - \hat{d}_{1 i}] \right]^{-\frac{1}{2}}
$$

Choose $ \alpha \in (\frac{3 + \delta}{20}, \frac{1}{6})$. Let $ h_{f} = n^{-\alpha}$, the base final bandwidth. Note that this base final bandwidth is designed to be smaller than the pilot bandwidth, since while capturing the whole data points is more important in pilot estimation, the final bandwidth should be small enough to capture more subtle details. Now calculate $\hat{g}_{1}(\cdot)$ based on above values:

$$
    \hat{g}_{1}(\cdot) = \frac{1}{n_{1}} \sum_{k} 1_{\{ Y_{k} \le j \}} \cdot K \left( \frac{V_{i} - V_{k}}{\hat{w}_{1i} \cdot h_{f} }\right) \cdot \frac{1}{\hat{w}_{1i} \cdot h_{f} } 
$$ 

$\hat{g}(\cdot)$ can be defined analogously, replacing $Y_k \leq j$ with $Y_k > j$. Note again that the final kernel estimation of $g_{1}(\cdot)$ is depend on the subsample of $V$ where $Y \le j$.

Even if we used local smoothing technique in estimation of the $\hat{p}$, there is still a concern that the some edge cases of $V$ may drag the estimation. In this situation trimming out some extreme data points can be a solution. Trimming function $\hat{m}$ does this by trimming out the data points whose absolute value are not within the range of $q$th quantile of $|X_{i}|$. 
$$
    \hat{m}(X_{i}) = 1_{\{ |X_{i}| < q\text{th quantile of } |X| \}}
$$ 

The final quasi-likelihood function in Equation (2) can now be constructed with above estimated values and the trimming function.

\begin{theorem}[Consistency of the estimator]
    For an consistent KDE, as $n \to \infty$, $\abs{\hat{\tau} - \tau} = o_{p}(1)$
\end{theorem}

The consistency of the estimates can be guaranteed by the fact that the KDE method is consistent. As the $n$ goes to infinity, the KDE estimates of $\hat{g}_{1}(\cdot)$ and $\hat{g}_{0}(\cdot)$ approach the true density and as the $\hat{\mathbb{P}\left(Y \le j\right)}$. Then the quasi-likelihood function approach the true likelihood function and the maximum likelihood then guarantees the consistency. The KDE method used in this estimator is locally smoothed KDE, and for the consistency of the locally smoothed KDE, see \citet{Klein1993a}, \citet{Abramson1982a}, and \citet{Silverman1986a}. 

\section{Monte Carlo Simulation}

I tried several Monte Carlo experiments to test the surrogate-based residuals and compare the performance of the Klein and Sherman semiparametric estimator compared with standard ordered probit and logit models. Here I consider the case where the outcome is a ordinal variable with 3 levels.

The latent variable $Y^{\ast}$ follows the relationship:
$$
    Y^{\ast} = D^{T} \tau + X^{T}\beta + \varepsilon
$$, and we observe $Y$ that is constructed as:
$$
    Y = \begin{cases}
        1 &\text{if}\quad \alpha_{-1} \le Y^\ast \le \alpha_0 \\
        2 &\text{if}\quad \alpha_{0} \le Y^\ast \le \alpha_1 \\
        3 &\text{if}\quad \alpha_{1} \le Y^\ast \le \alpha_2 \\
    \end{cases}
$$, where $\alpha_{-1} = -\infty$, and $\alpha_{2} = \infty$. 

$D$ simulates treatment status, so each data point is randomly assigned $0$(control) or $1$(treatment), $X$ is drawn from $\mathcal{N}(0, 25)$ and $\tau$ set to be $0.5$. 

I compare the performance of the KDE-based estimator introduced in Section 3 with OLS, ordered logit, and probit model. Since the ordinal regression estimators only identify $\tau$ up to scale or $\tau_{ALTE}$, without lose of generality, I set $\beta$ to be $1$ for easier interpretation. Thus, $X$ works as an anchored point here. Thus, $\tau_{ALTE} = \frac{\tau}{\beta} = 0.5$, which is same as the exact true $\tau$. This setting enables comparison across ordinal regression models (ordered logit, ordered probit, and KDE-based estimator), which identify $\tau_{ALTE}$ and OLS, which targets the exact $\tau$ based on cardinalization. For the OLS, a cardinalization of $Y$ is required. This paper follows the commonly used practice with labels of $\{1, 2, 3\}$. Bias ($\tau_{ALTE} - \hat{\tau}_{ALTE}$) and root mean squared error (RMSE, $\sqrt{\sum \frac{(\hat{\tau}_{ALTE} - \tau_{ALTE})^{2}}{n} }$) are used as performance metrics.
% The second cardinalization uses the labels of $\{1, 5 ,25\}$. The second cardinalization is expected to effectively increase the size of the estimated causal effect compared to that from the first cardinalization. 

I tested for two different distribution for the error term, $\varepsilon$, the standard normal distribution $\mathbb{N}(0, 1)$ and the t-distribution with degrees of freedom of 1. The t-distribution has heavier tail regions than both the standard logistic and the standard normal distribution. Thus, by construction, the normal error represents the most favorable DGP for the ordered probit model, and the t-distribution error term case represents DGP where the distributional assumptions for both ordered logit and ordered probit models are violated. Four sample sizes of 250, 500, 750 and 1,000 are considered. I first generate a population of size $1e+5$ and draw random samples for Monte Carlo replication from this population. The number of Monte Carlo replication is 1,000, and all estimations are the means of these 1,000 replication results. Regarding the KDE-based estimator, I used standard normal kernel and set $\delta = \frac{1}{15}$, pilot bandwidth to be 2/3th point of the suggested interval, the final base bandwidth to be the 2/5 point of the suggested interval, and fix all other parameters to the middle points of the suggested intervals. For instance, $\epsilon$ in the damping function is set to be $\frac{0 + (\frac{1}{40}- \frac{\delta}{20})}{2} \approx 0.33$. Also, the trimming was done at the 0.05 level, so data points within 0.05 quantile and 0.95. 

\begin{figure}
    \begin{tabular}{cc}
        \includegraphics[width=0.45\textwidth]{../figures/bias_norm.pdf} & \includegraphics[width=0.45\textwidth]{../figures/rmse_norm.pdf}\\
        (a) & (b)
    \end{tabular}
    \caption{Monte Carlo Simulation Results: Standard Normal Error}
\end{figure}

Figure 1 summarizes the Monte Carlo simulation results for the standard normal error term case.
First, in terms of root mean squared error (RMSE), Panel (b) shows that KDE-based estimator demonstrates only slight larger RMSE than other parametric ordinal regression models and comparable to that of OLS. Panel (a) shows the mean bias from 1,000 simulations of two OLS, ordered logit model, ordered probit model, and KDE-based estimator. The one notable thing is that OLS with common cardinalization of $\{1, 2, 3\}$ is significantly underestimating the $\tau$. However, OLS may also overestimate the effect if other cardinalization scheme is used. For example, OLS with $\{1, 5, 25\}$ cardinalization would blow up the coefficient and may lead to overestimation. Thus, the results from OLS models are highly dependent on the cardinalization schemes, and thus unreliable.

Both ordered probit model and ordered logit model show low estimated bias, and KDE-based estimator also shows comparably small bias across all sample sizes. Considering that the standard normal error case meets the distributional assumption for ordered probit model, the great performance of the ordered probit model is expected. Also, the CDF of standard logistic is not too different from that of standard normal, ordered logit model also retains small bias. KDE-based estimator also performs comparable to ordered probit model, only having slightly large bias when sample size is small.

\begin{figure}
    \begin{tabular}{cc}
        \includegraphics[width=0.45\textwidth]{../figures/bias_tdis.pdf} & \includegraphics[width=0.45\textwidth]{../figures/rmse_tdis.pdf}\\
        (a) & (b)
    \end{tabular}
    \caption{Monte Carlo Simulation Results: t-distribution Error}
\end{figure}

Figure 2 describes the results from the t-distribution error term case. Again, Panel (b) reports the RMSE for the four models, and KDE-based estimator shows relatively larger RMSE. On Panel (a),
OLS results show large underestimating bias. Ordered logit and ordered probit model suffer significant overestimation which is up to almost 10\% of the treatment effect. KDE estimator shows the best performance by controlling the bias at the lowest level.

Overall, the Monte Carlo simulation demonstrates that cardinalization approach depends on arbitrary labels and thus unreliable to capture the causal effect, standard ordered regression models may perform poorly when the distributional assumptions are violated. KDE-based estimator, however, demonstrates mostly better performance across two cases and different sample sizes.

\section{Applications: \citet{Tomz2020a}}
The example replicates the analysis by \citet{Tomz2020a}. The study looked into the question of how the human rights practices of foreign adversaries affect domestic public support for war through survey experiments. The main argument was that in democracies with well-established human rights, people are less willing to go to war with other countries that also respect human rights than with comparable countries that violate them. Thus they are focusing on two factors that can impact people's support for war: the political system of the foreign country and how much the foreign country respect human rights.

The paper uses total of 5 surveys, but this paper focus on the first survey, which contains an experiment for their main result. The survey was conducted by YouGov to a nationally representative sample of 1,430 US adults in October 2012. The treatment was given as in a form of vignettes on a country that is developing nuclear weapons. The vignettes also include trade and military relationship of the country and the US, and its conventional military strength which is set to the half of that of the US. In the vignettes, information about the political system (democracy = 1, not = 0) and human rights practices (respect = 1, not = 0) were binary treatments and they were randomized independently. 

The outcome of interest here is the support for physical strike of the US armed forces to the country. Specifically, respondents were asked to answer in 5-point scale: \textit{Favor strongly, Favor somewhat, Neither favor nor oppose, Oppose somewhat}, and \textit{Oppose strongly}. In the paper, the authors take the cardinalization approach and OLS. They first recode the outcome into binary variable, and assign numerical value of 0 if the answer was oppose and 100 otherwise. Then they used this to run OLS. They justify the cardinalization scheme by arguing that with the cardinalization, we can interpret the ATE as the percentage change in public support for a military strike as a result of learning the country's political system and human rights practices. However, as discussed in Section 2, the cardinalization is arbitrary and we only can interpret the ATE as suggested by authors when the cardinalization is capturing the unknown transformation function, which is untestable.

The model they used in their main results (model 2 of their Table 1) can be expressed as following:
$$
    Y^{\ast} = \tau_{DEM} \cdot DEM + \tau_{HR} \cdot HR + X^{T}\beta + \varepsilon 
$$ 
$$
    Y = \begin{cases}
        0 & \text{if \textit{Favor Strongly and Favor somewhat}} 
        100 & \text{Otherwise} 
    \end{cases}
$$ 
where $Y^{\ast}$ is the support for the military strike in latent scale and $Y$ is the observed outcome with their cardinalization, $DEM$ and $HR$ are binary variables indicating political system treatment and human rights practices treatment respectively, $X$ for the control variables and $\varepsilon$ for the error term.

Since they are interested in whether citizens living in democracy where human rights are respected (the US, to be more specific) are more reluctant to support military strike to a country who is developing nuclear weapons if the country also respecting human rights, the causal parameter we are interested in the above equation is $\tau_{HR}$. We only can observe the ordinal outcome, so only can identify the ALTE, thus I use the $DEM$ variable as an anchor. Thus our goal is to identify $\tau_{HR ALTE} = \frac{\tau_{HR}}{\tau_{DEM}}$. I replicate their result on Table 1 of their paper, and use ordered logit, ordered probit and KDE-based estimator as a comparison.

\begin{table}[!htb]
    \begin{center}
        \begin{tabular}{l c | c c c }
            \\ [-1.8ex]\hline\\ [-1.8ex]
            & ATE & \multicolumn{3}{c}{ALTE} \\
            \\ [-1.8ex]\cline{2-5} \\ [-1.8ex]
            & Original -- OLS & Ordered Logit & Ordered Probit & KDE-based \\
            \\ [-1.8ex] \hline\\ [-1.8ex]
            Respect HR & $-16.7$     & $-1.49$     & $-1.42$      & $-2.17$      \\
            & $(2.40)$     & $(0.10)$    & $(0.06)$     & $(0.27)$     \\
            \\ [-1.8ex] \hline\\ [-1.8ex]
            \multicolumn{5}{l}{\scriptsize{$^{***}p<0.001$; $^{**}p<0.01$; $^{*}p<0.05$}}
        \end{tabular}
    \end{center}
    \caption{Replication of \citet{Tomz2020a} Table 3}
\end{table}

Table 1 reports the results from OLS, ordered logit, ordered probit and KDE-based method. Overall, all methods agree on the direction of the treatment, that learning that a nuclear weapon developing country is respecting human rights decreases the over all support for the military attack. However, the interpretations of the estimated treatment effect are different. Since the original OLS with binarized outcome is assuming cardinalization mentioned above the coefficient is aiming to estimate the ATE directly. The authors interpret the coefficient of $-16.7$ as respondents become on average 16.7 percentage points less likely to support the military attack when they were told that the country is respecting human rights. However, this is only true when the cardinalization is true. 

Other models identifies the ALTE of learning human rights practices with respect to that of the political system of the country, but the estimated sizes of ALTE vary across models. The results from ordered logit model and ordered probit model suggest somewhat similar size of ALTE; $- 1.49$ and $-1.42$ respectively. However, since both models rely on strong distributional assumption on the error term, $\varepsilon$, and thus the estimation may inconsistent and biased. This seems indeed true for this case, that the result from KDE-based method suggest that the ALTE may be much larger as $-2.17$. 

\section{Conclusion}

This paper has examined the challenge of identifying causal effects when outcomes are measured on ordinal scales, a setting that arises frequently in political science research. Survey experiments, opinion polls, and observational studies routinely rely on ordinal measures such as Likert responses, approval ratings, and policy preferences. Despite their prevalence, the methodological implications of ordinal data for causal inference have often been overlooked. Researchers commonly treat ordinal responses as cardinal or employ ordered logit and probit models, but both strategies can be problematic. Cardinalization imposes arbitrary structure on the data, obscuring the true magnitude of treatment effects, while ordered regression models rely on strong distributional assumptions that, when violated, yield biased or even misleading results.

This study makes two contributions to addressing these issues. First, it clarifies the limits of identification in the ordinal setting: even under standard assumptions such as SUTVA, ignorability, and positivity, treatment effects on the latent variable can only be identified up to scale. This insight is important not just technically, but also substantively -- it underscores that researchers must be cautious about interpreting the estimated quantity from regressions with ordinal outcomes, as what is being estimated is inherently restricted. Second, the paper proposes a semiparametric alternative to conventional ordered models. By employing a kernel density estimation (KDE)–based  approach, the estimator avoids restrictive distributional assumptions while maintaining interpretability in terms of treatment effects on the latent scale.

The evidence presented here suggests that semiparametric methods can meaningfully improve inference in practice. Monte Carlo simulations reveal that the KDE estimator remains robust to misspecification of the error distribution, while OLS and ordered regression models fail under such conditions. The empirical application further illustrates that reliance on conventional approaches can substantively alter conclusions about treatment effects. Specifically, the reanalysis of Tomz (2020) shows the limitation of OLS, and suggest possible underestimation of causal effects in ordered logit and probit models compared to the proposed estimator. This highlights how methodological choices in handling ordinal outcomes can shape the interpretation of politically relevant findings.

For applied researchers, these results carry two main lessons. First, the pervasiveness of ordinal outcomes in political science makes it essential to acknowledge their limitations for causal inference. Treating ordinal measures as cardinal should be avoided, and researchers should be mindful of the strict assumptions embedded in ordered regression models. Second, semiparametric methods such as the KDE estimator offer a viable and accessible alternative, allowing for robust estimation without sacrificing interpretability. In this way, the approach provides a bridge between the need for causal identification and the practical realities of working with ordinal data.

At the same time, this paper opens several avenues for further work. One obvious direction is to explore the other methods of density estimation that can be an alternative to KDE in suggested estimator. The performance of semiparametric estimator depends highly on the quality of the density estimation, so if one can come up with a density estimation method with better properties, this would greatly improve the whole estimation process. Another is to extend these tools to settings with multilevel data, treatment effect heterogeneity, or dynamic outcomes, which are common in political science applications. Finally, while this paper has emphasized the estimation of average treatment effects, future work could investigate the identification of other causal quantities -- such as distributional shifts -- when outcomes are ordinal.

\bibliographystyle{apsr}
\bibliography{/Users/chanhyuk/Documents/MyLibrary}
\end{document}
