%vim:foldmethod=marker
%{{{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Define Article %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[10pt]{extarticle}
% \synctex=1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Using Packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{geometry}
\geometry{
    letterpaper,
    % landscape,
    % margin=1in
}
% \usepackage{kotex}
\usepackage{luaquotes}
\usepackage{natbib}
\usepackage[hidelinks]{hyperref}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{unicode-math}
\usepackage{booktabs}
\usepackage{tabularx}
% \usepackage{adjustbox}
% \usepackage{rotating}
% \usepackage{caption}
% \captionsetup[table]{skip=10pt}

\usepackage{graphicx}
% \usepackage{listings}
% \usepackage{tikz}

\usepackage{setspace}
\doublespacing
% }}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Title & Author %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Some Ideas}
\author{Chanhyuk Park}
\date{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
    \maketitle

\section{Test for Ignorability}
One of the key assumptions for causal inference is the (statistical) independence between the (potential) outcome, $Y(t)$ and the treatment status, $T = t$. For most of the observational studies, this independence or ignorability assumption is stated conditioning on some of other control variables, $X$, which theoretically may bridge the relationship between $Y$ and $T$, other than $T$'s direct effect on $Y$, which we are interested in. This can be formally specified in conditional independence of $Y(t) \perp\!\!\!\perp T \mid X$. Unaccounted confounding variables, measurement errors, and wrongly specified models can violate the assumption. However, it is hard to test for the assumption to be met. \citet{Donald2014a,Luna2014a} suggested methods exploiting the existence of an instrumental variables, \citet{Chen2018b} proposed a Kolmogorovâ€“Smirnov type test, which compares an estimator that is only valid with conditional mean independence assumption and the other without it. \citet{Fang2020a} uses auxiliary variable to test the assumption. They derived some moment conditions that should be met if the conditional unconfounded assumption holds. 

Here I suggest a different approach to test for the conditional independence assumption, using the burgeoning studies on Gaussianization through normalizing flows. Normalizing flows aim to find a invertible function which transforms a simple base density into a more complex density function. \citet{Chen2000a} provides theoretical basis for the existence of such an invertible function. The key here is that the function is invertible: if we can find the function that transforms Gaussian density to the target densities we want to know, then, it can be easier to test for the independence. More specifically, if we can find conditional normalizing flows for each $Y \mid X$ and $T \mid X$, and then we can express them in terms of Gaussian distribution, which can open some easier way to test for the conditional independence between $Y$ and $T$. For example, simple correlation test might work. 
%
One short coming is that finding conditional normalizing flows usually involves machine learning, where size and quality of the data matters. Also, the test cannot tell the possible reason for the violation. 

\newpage
\section{Partial Identification Using Moment (In)equalities and Normalizing Flows}
To point identified the parameters of interests, researchers usually implicitly or explicitly impose some set of assumptions. Some of the assumptions sometimes questionable, but usually left untested. The research project of partial identification suggests another way of identification of the parameters of interests; instead of point identify them with doubtful assumptions, it aims provide informative bounds for the parameters with limited sets of assumptions \citet{Manski1990a}. The point here is that some justifiable set of assumptions may be enough to provide identification of the parameters. 

Frameworks have been suggested to bound the parameters. One strand of research focuses on the moment (in)equalities that should be met under the assumptions and data, similar to the generalized method of moments approach. \citet{Andrews2010a, Schennach2014a}, and \citet{Ho2015a} are in this framework, and reviewed in \citet{Canay2017a}. \cite{Chernozhukov2007a} provides bootstrap based estimation of the identified set with moment inequalities. The problem here is that in some cases, it is hard to find out all the moment inequalities. Also, the continuity of the finite sample estimator \citep{Kline2023a}. Without further assumption of the continuous estimation, the approach may fail to identify the bounds,even though the true identified set is not empty. Another approach exploits the random set theory. Since the identified set (the bounds) should be a random set, it is natural to connect the partial identification with theories on random sets. To name a few, \citet{Molchanov2005a, Molinari2010a, Beresteanu2012a}, and \citet{Molchanov2018a} deals with the idea. The approach actively utilizes Aumann expectation, which is the set of expectation of the elements. The shortcoming of the random set approach is that it is hard to derive the bounds if the true identified set is happen to be non-convex. 

The problem with partial identification is that the most of the required tasks do not have analytic solutions \citep{Molinari2020a, Schennach2020a}. I think here the machine learning techniques can improve the situation. For example, \citet{Schennach2014a} suggests a framework using Entropic Latent Variable Integration via Simulation (ELVIS), which basically integrate the moment condition based on the least likely probability measure for the unobservables. He proposed to select the least likely probability in terms of (relative) entropy. However, we may again exploit normalizing flows, to find the most likely density of the unobservables, or the conditional densities of $T$ and $Y$ that are independent to account for unobservables. 

\newpage
\section{Sensitivity Analysis for Linear Models}
Sensitivity analysis provides ways to test how sensitive the result from an analysis is to a certain assumption. Usually, it provides to what extend a assumption can be relaxed. In this sense, we can think of this as a bridge between the partial identification approach. Rosenbaum Bounds \citep{Rosenbaum1987a}, Marginal Sensitivity Model (MSM) \citep{Tan2006a}, and f-sensitivity models \citep{Jin2022a}. Among them, MSM have gotten a lot of attention \citep{Kallus2019a, Zhao2019a}. \citet{Frauen2023a} provides generalized MSM, which specifically dealt with the continuous and time-varying treatment cases. The shortcoming of MSM is that it requires researcher to decide the distribution of the unobservables.  The sensitivity analysis in general involves the sensitivity parameter $\Gamma$, which controls the strength of unobserved confounding, where $\Gamma = 1$ corresponds with no unconfoundedness, and thus the model can be point identified. $\Gamma$ can typically be chosen by domain knowledge or data-driven heuristics \citep{Kallus2019a}, or many provides contour graphs of the results for the different values of $\Gamma$. 

Little works on political science field explores the sensitivity analysis, \citet{Imai2010a} followed Manski's works to provide sensitivity analysis and bounds for the differential errors in surveys, and \citet{Mebane2013a} exploited Molinari's extension of Manski Bounds with Bayesian bootstrap method. I may introduce recently developed MSM methods to the field.


\bibliographystyle{apsr}
\bibliography{/Users/chanhyuk/Documents/MyLibrary}
\end{document}
